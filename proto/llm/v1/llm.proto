// File: proto/llm/v1/llm.proto
// Defines a provider-agnostic interface for interacting with Large Language Models.
syntax = "proto3";

package llm.v1;

import "common/v1/common.proto";
import "google/protobuf/struct.proto";

option go_package = "github.com/spounge-ai/spounge-protos/gen/go/llm/v1;llmv1";

// Supported LLM providers.
enum Provider {
  PROVIDER_UNSPECIFIED = 0;
  PROVIDER_OPENAI = 1;
  PROVIDER_ANTHROPIC = 2;
  PROVIDER_GOOGLE = 3;
  PROVIDER_AZURE_OPENAI = 4;
  PROVIDER_COHERE = 5;
  PROVIDER_OLLAMA = 6;
}

// Role of the message author in a conversation.
enum MessageRole {
  MESSAGE_ROLE_UNSPECIFIED = 0;
  MESSAGE_ROLE_SYSTEM = 1;
  MESSAGE_ROLE_USER = 2;
  MESSAGE_ROLE_ASSISTANT = 3;
  MESSAGE_ROLE_TOOL = 4;
}

// A single message in a conversation.
message Message {
  MessageRole role = 1;
  string content = 2;
  optional string name = 3; // For tool roles
}

// Definition of a tool the model can call (function calling).
message Tool {
  string name = 1;
  string description = 2;
  // JSON schema for the tool's parameters.
  google.protobuf.Struct parameters = 3;
}

// A tool call generated by the model.
message ToolCall {
  string id = 1; // ID for the tool call
  string name = 2;
  // The arguments for the tool, as a JSON object string.
  string arguments = 3;
}

// Common generation parameters.
message GenerationParams {
  optional float temperature = 1;
  optional float top_p = 2;
  optional int32 max_tokens = 3;
  repeated string stop_sequences = 4;
  optional bool stream = 5;
}

// Token usage statistics for a generation request.
message TokenUsage {
  int32 prompt_tokens = 1;
  int32 completion_tokens = 2;
  int32 total_tokens = 3;
}

// Request to generate text.
message GenerateTextRequest {
  Provider provider = 1;
  string model = 2;
  repeated Message messages = 3;
  optional GenerationParams params = 4;
  repeated Tool tools = 5;
  // Metadata passed from the calling context.
  common.v1.Metadata metadata = 6;
}

// Response from a text generation request.
message GenerateTextResponse {
  common.v1.Status status = 1;
  Message response_message = 2;
  repeated ToolCall tool_calls = 3;
  optional TokenUsage usage = 4;
  // Reason the generation finished, e.g., "stop", "length", "tool_calls".
  string finish_reason = 5;
}

// Streaming text generation response chunk.
message GenerateTextStreamResponse {
  common.v1.Status status = 1;
  // The incremental text delta.
  string delta = 2;
  optional TokenUsage usage = 3; // Only set in the final message.
  optional string finish_reason = 4; // Only set in the final message.
  bool is_final = 5;
}

// Request to generate embeddings for a list of texts.
message GenerateEmbeddingRequest {
  Provider provider = 1;
  string model = 2;
  repeated string texts = 3;
  common.v1.Metadata metadata = 4;
}

message Embedding {
  repeated float values = 1;
  int32 dimension = 2;
}

message GenerateEmbeddingResponse {
  common.v1.Status status = 1;
  repeated Embedding embeddings = 2;
  optional TokenUsage usage = 3;
}

// LLMProviderService is an internal service, typically called by PolykeyService.
service LLMProviderService {
  rpc GenerateText(GenerateTextRequest) returns (GenerateTextResponse) {}
  rpc GenerateTextStream(GenerateTextRequest) returns (stream GenerateTextStreamResponse) {}
  rpc GenerateEmbedding(GenerateEmbeddingRequest) returns (GenerateEmbeddingResponse) {}
}