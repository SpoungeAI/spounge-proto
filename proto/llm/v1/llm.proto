syntax = "proto3";

package llm.v1;

import "common/v1/common.proto";
import "google/protobuf/struct.proto";

option go_package = "github.com/spounge-ai/spounge-protos/gen/go/llm/v1;llmv1";

enum Provider {
  PROVIDER_UNSPECIFIED = 0;
  PROVIDER_OPENAI = 1;
  PROVIDER_ANTHROPIC = 2;
  PROVIDER_GOOGLE = 3;
  PROVIDER_AZURE_OPENAI = 4;
  PROVIDER_COHERE = 5;
  PROVIDER_OLLAMA = 6;
}

enum MessageRole {
  MESSAGE_ROLE_UNSPECIFIED = 0;
  MESSAGE_ROLE_SYSTEM = 1;
  MESSAGE_ROLE_USER = 2;
  MESSAGE_ROLE_ASSISTANT = 3;
  MESSAGE_ROLE_TOOL = 4;
}

message Message {
  MessageRole role = 1;
  string content = 2;
  optional string name = 3;
}

message Tool {
  string name = 1;
  string description = 2;
  google.protobuf.Struct parameters = 3;
}

message ToolCall {
  string id = 1;
  string name = 2;
  string arguments = 3;
}

message GenerationParams {
  optional float temperature = 1;
  optional float top_p = 2;
  optional int32 max_tokens = 3;
  repeated string stop_sequences = 4;
  optional bool stream = 5;
}

message TokenUsage {
  int32 prompt_tokens = 1;
  int32 completion_tokens = 2;
  int32 total_tokens = 3;
}

message GenerateTextRequest {
  Provider provider = 1;
  string model = 2;
  repeated Message messages = 3;
  optional GenerationParams params = 4;
  repeated Tool tools = 5;
  common.v1.Metadata metadata = 6;
}

message GenerateTextResponse {
  common.v1.Status status = 1;
  Message response_message = 2;
  repeated ToolCall tool_calls = 3;
  optional TokenUsage usage = 4;
  string finish_reason = 5;
}

// Request to generate text in a streaming fashion.
message GenerateTextStreamRequest {
  GenerateTextRequest request = 1;
}

message GenerateTextStreamResponse {
  common.v1.Status status = 1;
  string delta = 2;
  optional TokenUsage usage = 3;
  optional string finish_reason = 4;
  bool is_final = 5;
}

message GenerateEmbeddingRequest {
  Provider provider = 1;
  string model = 2;
  repeated string texts = 3;
  common.v1.Metadata metadata = 4;
}

message Embedding {
  repeated float values = 1;
  int32 dimension = 2;
}

message GenerateEmbeddingResponse {
  common.v1.Status status = 1;
  repeated Embedding embeddings = 2;
  optional TokenUsage usage = 3;
}

service LLMProviderService {
  rpc GenerateText(GenerateTextRequest) returns (GenerateTextResponse) {}
  rpc GenerateTextStream(GenerateTextStreamRequest) returns (stream GenerateTextStreamResponse) {}
  rpc GenerateEmbedding(GenerateEmbeddingRequest) returns (GenerateEmbeddingResponse) {}
}
